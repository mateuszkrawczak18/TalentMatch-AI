import sys
import os
import time
import concurrent.futures
from dotenv import load_dotenv

# --- MAGICZNY NAG≈Å√ìWEK ---
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

# ≈Åadowanie .env
load_dotenv(os.path.join(parent_dir, ".env"))

# Import Silnika (z g≈Ç√≥wnego folderu)
from bi_engine import BusinessIntelligenceEngine

def run_load_test():
    print("\n" + "="*70)
    print("üö¶ THROUGHPUT TEST: Simulating 100 concurrent users")
    print("="*70)

    # 1. Inicjalizacja silnika
    try:
        engine = BusinessIntelligenceEngine()
        print("‚úÖ Engine initialized. Starting attack...")
    except Exception as e:
        print(f"‚ùå Engine init failed: {e}")
        return

    # 2. Funkcja symulujƒÖca jednego u≈ºytkownika
    def simulate_user_request(user_id):
        try:
            start = time.time()
            # Wykonujemy "lekkie" zapytanie bezpo≈õrednio do grafu, 
            # ≈ºeby przetestowaƒá wydajno≈õƒá po≈ÇƒÖcze≈Ñ (Connection Pool)
            # Omijamy LLM, ≈ºeby nie dostaƒá b≈Çƒôdu 429 (Too Many Requests) od OpenAI
            engine.graph.query("MATCH (p:Person) RETURN count(p)")
            end = time.time()
            return True, end - start
        except Exception as e:
            return False, str(e)

    # 3. Uruchomienie 100 wƒÖtk√≥w na raz
    TOTAL_REQUESTS = 100
    CONCURRENT_THREADS = 20 # Typowy limit dla lokalnego laptopa
    
    start_test = time.time()
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=CONCURRENT_THREADS) as executor:
        # Odpalamy 100 zada≈Ñ
        results = list(executor.map(simulate_user_request, range(TOTAL_REQUESTS)))
    
    end_test = time.time()
    total_duration = end_test - start_test

    # 4. Analiza wynik√≥w
    success_count = sum(1 for r in results if r[0])
    failures = [r[1] for r in results if not r[0]]
    avg_latency = sum(r[1] for r in results if r[0]) / success_count if success_count > 0 else 0
    throughput = TOTAL_REQUESTS / total_duration

    print(f"\nüìä RESULTS:")
    print(f"   - Total Requests: {TOTAL_REQUESTS}")
    print(f"   - Successful:     {success_count} ‚úÖ")
    print(f"   - Failed:         {len(failures)} ‚ùå")
    print(f"   - Total Time:     {total_duration:.2f}s")
    print(f"   - Avg Latency:    {avg_latency:.4f}s")
    print(f"   - Throughput:     {throughput:.2f} req/sec")

    if success_count == TOTAL_REQUESTS:
        print("\n‚úÖ SUCCESS: System handled 100 concurrent requests without crashing.")
        print("   (Note: We tested DB/Backend throughput. LLM API limits are external constraints.)")
    else:
        print("\n‚ö†Ô∏è WARNING: Some requests failed.")
        if failures:
            print(f"   Reason: {failures[0]}")

if __name__ == "__main__":
    run_load_test()